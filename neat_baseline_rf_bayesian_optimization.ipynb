{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is pretty much a fork of the following neat and clean kernel :\n",
    "\n",
    "https://www.kaggle.com/byfone/neat-baseline-code-for-earthquake-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# to import the training data efficiently by a chunksize=150000\n",
    "# why 150000? becaues it is identified that each test file contains 150000 rows of data\n",
    "# it is also verified that loading the training 'acoustic_data' to int16 \n",
    "# and 'time_to_failure' to float32 can drastically reduce the memory usage\n",
    "trainDF = pd.read_csv('train.csv', iterator=True, chunksize=150000, dtype={'acoustic_data':np.int16, 'time_to_failure':np.float32})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       acoustic_data  time_to_failure\n",
      "count  150000.000000    150000.000000\n",
      "mean        4.884113         1.450068\n",
      "std         5.101106         0.011248\n",
      "min       -98.000000         1.430797\n",
      "25%         3.000000         1.440398\n",
      "50%         5.000000         1.449999\n",
      "75%         7.000000         1.459599\n",
      "max       104.000000         1.469100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4691"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.describe())\n",
    "x.max().values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature generator\n",
    "# at the first attempt, let's just use the simple statistic properties of the training time series segment (150000 units)\n",
    "\n",
    "def feature_generator(x):\n",
    "    \"\"\" \n",
    "        Input: Pandas data series, in this case the 'acoustic_data' \n",
    "        OUtput: Pandas data series, in this case the features\n",
    "    \"\"\"\n",
    "     \n",
    "    features = {'mean': x.mean(),\n",
    "                'std': x.std(),\n",
    "                'max': x.max(),\n",
    "                'min': x.min(),\n",
    "                'q25': x.quantile(.25),\n",
    "                'q50': x.quantile(.5),\n",
    "                'q75': x.quantile(.75),\n",
    "                'iqr': x.quantile(.75) - x.quantile(.25),\n",
    "                'mad': x.mad(),\n",
    "                'skew': x.skew(),\n",
    "                'kurtosis': x.kurtosis()\n",
    "    }\n",
    "    \n",
    "    fseries = pd.Series(features)\n",
    "    return fseries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to reconstruct the training data and testing data into X_train and y_train format\n",
    "\n",
    "def xy_train_generator(xy):\n",
    "    \"\"\" \n",
    "        input: the chunk of 150000 dataframe\n",
    "        output: X_train, y_train that is ready for the sklearn machine learning models, where\n",
    "                X_train is a Data Frame\n",
    "                y_Train is a Data Series\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the feature names and drop the first segment (chunksize of 150000 record)\n",
    "    segment = next(xy)\n",
    "    cols = feature_generator(segment['acoustic_data']).index\n",
    "    \n",
    "    X_train = pd.DataFrame(columns=cols)\n",
    "    y_train = pd.Series()\n",
    "    \n",
    "    for xxyy in tqdm(xy):\n",
    "        X_train = X_train.append(feature_generator(xxyy['acoustic_data']), ignore_index=True)\n",
    "        y_train = y_train.append(pd.Series(xxyy['time_to_failure'].values[-1]))\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0443763ac354db990fc2db6c7a05492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = xy_train_generator(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>q25</th>\n",
       "      <th>q50</th>\n",
       "      <th>q75</th>\n",
       "      <th>iqr</th>\n",
       "      <th>mad</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.906393</td>\n",
       "      <td>6.967397</td>\n",
       "      <td>140.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.948411</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>33.555211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.902240</td>\n",
       "      <td>6.922305</td>\n",
       "      <td>197.0</td>\n",
       "      <td>-199.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.647117</td>\n",
       "      <td>0.757278</td>\n",
       "      <td>116.548172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.908720</td>\n",
       "      <td>7.301110</td>\n",
       "      <td>145.0</td>\n",
       "      <td>-126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.826052</td>\n",
       "      <td>0.064531</td>\n",
       "      <td>52.977905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.913513</td>\n",
       "      <td>5.434111</td>\n",
       "      <td>142.0</td>\n",
       "      <td>-144.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.378388</td>\n",
       "      <td>-0.100697</td>\n",
       "      <td>50.215147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.855660</td>\n",
       "      <td>5.687823</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.560799</td>\n",
       "      <td>0.208810</td>\n",
       "      <td>23.173004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       std    max    min  q25  q50  q75  iqr       mad      skew  \\\n",
       "0  4.906393  6.967397  140.0 -106.0  2.0  5.0  7.0  5.0  3.948411  0.217391   \n",
       "1  4.902240  6.922305  197.0 -199.0  2.0  5.0  7.0  5.0  3.647117  0.757278   \n",
       "2  4.908720  7.301110  145.0 -126.0  2.0  5.0  7.0  5.0  3.826052  0.064531   \n",
       "3  4.913513  5.434111  142.0 -144.0  3.0  5.0  7.0  4.0  3.378388 -0.100697   \n",
       "4  4.855660  5.687823  120.0  -78.0  2.0  5.0  7.0  5.0  3.560799  0.208810   \n",
       "\n",
       "     kurtosis  \n",
       "0   33.555211  \n",
       "1  116.548172  \n",
       "2   52.977905  \n",
       "3   50.215147  \n",
       "4   23.173004  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a peak of the X_train data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.353196\n",
       "0    1.313798\n",
       "0    1.274400\n",
       "0    1.236097\n",
       "0    1.196798\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a peak of the y_train data\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is always a good practice to scale the input data, let's use the Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>q25</th>\n",
       "      <th>q50</th>\n",
       "      <th>q75</th>\n",
       "      <th>iqr</th>\n",
       "      <th>mad</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.511941</td>\n",
       "      <td>0.049298</td>\n",
       "      <td>-0.086211</td>\n",
       "      <td>0.162954</td>\n",
       "      <td>-0.405908</td>\n",
       "      <td>0.947251</td>\n",
       "      <td>0.288859</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.191632</td>\n",
       "      <td>-0.492558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.495717</td>\n",
       "      <td>0.043996</td>\n",
       "      <td>0.122610</td>\n",
       "      <td>-0.187833</td>\n",
       "      <td>-0.405908</td>\n",
       "      <td>0.947251</td>\n",
       "      <td>0.288859</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.101267</td>\n",
       "      <td>1.321256</td>\n",
       "      <td>0.684023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.521030</td>\n",
       "      <td>0.088535</td>\n",
       "      <td>-0.067894</td>\n",
       "      <td>0.087516</td>\n",
       "      <td>-0.405908</td>\n",
       "      <td>0.947251</td>\n",
       "      <td>0.288859</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.211630</td>\n",
       "      <td>-0.128201</td>\n",
       "      <td>-0.217205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.539754</td>\n",
       "      <td>-0.130984</td>\n",
       "      <td>-0.078884</td>\n",
       "      <td>0.019621</td>\n",
       "      <td>1.594060</td>\n",
       "      <td>0.947251</td>\n",
       "      <td>0.288859</td>\n",
       "      <td>-0.917989</td>\n",
       "      <td>-0.064479</td>\n",
       "      <td>-0.473912</td>\n",
       "      <td>-0.256373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.313763</td>\n",
       "      <td>-0.101153</td>\n",
       "      <td>-0.159482</td>\n",
       "      <td>0.268567</td>\n",
       "      <td>-0.405908</td>\n",
       "      <td>0.947251</td>\n",
       "      <td>0.288859</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.048028</td>\n",
       "      <td>0.173679</td>\n",
       "      <td>-0.639746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       std       max       min       q25       q50       q75  \\\n",
       "0  1.511941  0.049298 -0.086211  0.162954 -0.405908  0.947251  0.288859   \n",
       "1  1.495717  0.043996  0.122610 -0.187833 -0.405908  0.947251  0.288859   \n",
       "2  1.521030  0.088535 -0.067894  0.087516 -0.405908  0.947251  0.288859   \n",
       "3  1.539754 -0.130984 -0.078884  0.019621  1.594060  0.947251  0.288859   \n",
       "4  1.313763 -0.101153 -0.159482  0.268567 -0.405908  0.947251  0.288859   \n",
       "\n",
       "        iqr       mad      skew  kurtosis  \n",
       "0  0.474100  0.287099  0.191632 -0.492558  \n",
       "1  0.474100  0.101267  1.321256  0.684023  \n",
       "2  0.474100  0.211630 -0.128201 -0.217205  \n",
       "3 -0.917989 -0.064479 -0.473912 -0.256373  \n",
       "4  0.474100  0.048028  0.173679 -0.639746  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look of the after scaling X_train data\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3354, 11) (3354,)\n",
      "(839, 11) (839,)\n"
     ]
    }
   ],
   "source": [
    "# now time to get our hands dirty, let's implement a very simple random forest model and check our score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "print(XX_train.shape, yy_train.shape)\n",
    "print(XX_test.shape, yy_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(XX_train, yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.153989798469502\n"
     ]
    }
   ],
   "source": [
    "# find the mse when predicting with the test dataset\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "yy_pred = rf.predict(XX_test)\n",
    "MAE = mean_absolute_error(yy_test, yy_pred)\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "The remainig of this notebook is all about tuning, using random forecast as example\n",
    "Some reference URLs:\n",
    "- https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 1000,\n",
      " 'n_jobs': 1,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Let's check our current rf parameters\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 16.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model for hyper parameter tuning\n",
    "rf_base = RandomForestRegressor()  \n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf_base, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(XX_train, yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'max_depth': 10,\n",
      " 'max_features': 'sqrt',\n",
      " 'min_samples_leaf': 4,\n",
      " 'min_samples_split': 2,\n",
      " 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# let's see the best parameters from fitting the random search\n",
    "pprint(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1203438304587183\n"
     ]
    }
   ],
   "source": [
    "# also, let's see how does the best rf perform\n",
    "best_rf = rf_random.best_estimator_\n",
    "yy_pred = best_rf.predict(XX_test)\n",
    "MAE = mean_absolute_error(yy_test, yy_pred)\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with scikit-learn\n",
    "\n",
    "reference: \n",
    "- https://thuijskens.github.io/2016/12/29/bayesian-optimisation/\n",
    "- https://github.com/thuijskens/bayesian-optimization\n",
    "- https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random forest cross validation function\n",
    "# should generalize this function to take it most of the sklearn models\n",
    "\n",
    "def rfc_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf, input_data, target):\n",
    "    \"\"\"\n",
    "        Random Forest cross validation.\n",
    "        This function will instantiate a random forest classifier with parameters\n",
    "        interested parameters. By feeding the data and paramters to the model, the resulting targets \n",
    "        will be used to perform cross validation. The result of cross validation is then returned.\n",
    "        \n",
    "        The goal is to find combinations of interested parameters that minimzes the log loss.\n",
    "    \"\"\"\n",
    "        \n",
    "    estimator = RandomForestRegressor(\n",
    "        n_estimators= n_estimators,\n",
    "        max_features = 'sqrt', \n",
    "        max_depth = max_depth, \n",
    "        min_samples_split = min_samples_split,\n",
    "        min_samples_leaf = min_samples_leaf, \n",
    "        bootstrap = True, \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cval = cross_val_score(estimator, input_data, target, scoring='neg_mean_absolute_error', cv=3)\n",
    "    \n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a function to tune the random forecst paramters using bayesian optimization\n",
    "\n",
    "def bOpt_rfc(input_data, target):\n",
    "    \"\"\"Apply Bayesian Optimization to Random Forest parameters.\"\"\"\n",
    "    def rfc_crossval(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
    "        \"\"\"Wrapper of RandomForest cross validation.\n",
    "        Notice how we ensure n_estimators and min_samples_split are casted\n",
    "        to integer before we pass them along. Moreover, to avoid max_features\n",
    "        taking values outside the (0, 1) range, we also ensure it is capped\n",
    "        accordingly.\n",
    "        \"\"\"\n",
    "        r = rfc_cv(n_estimators=int(n_estimators), max_depth = int(max_depth), min_samples_split=int(min_samples_split), \\\n",
    "            min_samples_leaf=int(min_samples_leaf), input_data=input_data, target=target)\n",
    "        return r\n",
    "    \n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=rfc_crossval,\n",
    "        pbounds={\n",
    "            'n_estimators': (200, 2000),\n",
    "            'max_depth': (10,110),\n",
    "            'min_samples_split': (2, 10),\n",
    "            'min_samples_leaf': (1, 4)\n",
    "        },\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "\n",
    "    optimizer.maximize(init_points=2, n_iter=50)\n",
    "\n",
    "    print(\"Final result:\", optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "|  1        | -2.177    |  47.45    |  3.852    |  7.856    |  1.278e+0 |\n",
      "|  2        | -2.192    |  25.6     |  1.468    |  2.465    |  1.759e+0 |\n",
      "|  3        | -2.17     |  10.65    |  1.56     |  3.404    |  200.8    |\n",
      "|  4        | -2.193    |  109.9    |  1.483    |  5.008    |  206.5    |\n",
      "|  5        | -2.168    |  10.04    |  1.017    |  3.06     |  711.7    |\n",
      "|  6        | -2.165    |  10.29    |  3.975    |  9.983    |  1.066e+0 |\n",
      "|  7        | -2.169    |  10.43    |  1.274    |  2.019    |  1.09e+03 |\n",
      "|  8        | -2.165    |  10.02    |  3.217    |  9.937    |  808.1    |\n",
      "|  9        | -2.165    |  10.01    |  3.345    |  9.847    |  887.1    |\n",
      "|  10       | -2.165    |  10.39    |  3.946    |  9.872    |  930.3    |\n",
      "|  11       | -2.165    |  10.08    |  3.83     |  9.965    |  777.5    |\n",
      "|  12       | -2.165    |  10.19    |  3.823    |  9.988    |  535.4    |\n",
      "|  13       | -2.165    |  10.06    |  3.962    |  9.938    |  675.1    |\n",
      "|  14       | -2.166    |  10.2     |  3.483    |  9.985    |  487.2    |\n",
      "|  15       | -2.167    |  10.01    |  2.824    |  9.948    |  852.7    |\n",
      "|  16       | -2.164    |  10.03    |  3.625    |  9.913    |  971.1    |\n",
      "|  17       | -2.167    |  10.22    |  1.792    |  9.998    |  1.05e+03 |\n",
      "|  18       | -2.166    |  10.16    |  3.466    |  9.764    |  501.5    |\n",
      "|  19       | -2.165    |  10.1     |  3.997    |  9.502    |  722.7    |\n",
      "|  20       | -2.165    |  10.55    |  3.971    |  9.992    |  579.4    |\n",
      "|  21       | -2.165    |  10.25    |  3.976    |  9.83     |  825.0    |\n",
      "|  22       | -2.165    |  10.11    |  3.989    |  9.665    |  944.1    |\n",
      "|  23       | -2.165    |  10.36    |  3.914    |  9.89     |  598.4    |\n",
      "|  24       | -2.165    |  10.16    |  3.964    |  9.43     |  729.7    |\n",
      "|  25       | -2.165    |  10.28    |  3.858    |  9.667    |  747.1    |\n",
      "|  26       | -2.165    |  10.24    |  3.894    |  9.738    |  565.6    |\n",
      "|  27       | -2.168    |  10.09    |  1.43     |  9.999    |  487.2    |\n",
      "|  28       | -2.165    |  10.21    |  3.723    |  9.506    |  923.0    |\n",
      "|  29       | -2.165    |  10.48    |  3.676    |  9.856    |  976.2    |\n",
      "|  30       | -2.164    |  10.3     |  3.973    |  9.924    |  1.121e+0 |\n",
      "|  31       | -2.164    |  10.27    |  3.917    |  9.731    |  1.166e+0 |\n",
      "|  32       | -2.165    |  10.26    |  3.875    |  9.548    |  1.105e+0 |\n",
      "|  33       | -2.164    |  10.17    |  3.558    |  9.937    |  1.265e+0 |\n",
      "|  34       | -2.165    |  10.01    |  3.983    |  9.444    |  1.188e+0 |\n",
      "|  35       | -2.165    |  10.04    |  3.745    |  9.26     |  1.217e+0 |\n",
      "|  36       | -2.164    |  10.28    |  3.924    |  9.777    |  1.33e+03 |\n",
      "|  37       | -2.167    |  11.27    |  3.856    |  9.69     |  1.297e+0 |\n",
      "|  38       | -2.165    |  10.92    |  3.748    |  9.674    |  1.191e+0 |\n",
      "|  39       | -2.164    |  10.34    |  3.99     |  9.716    |  1.273e+0 |\n",
      "|  40       | -2.164    |  10.49    |  3.469    |  9.564    |  1.147e+0 |\n",
      "|  41       | -2.164    |  10.02    |  3.988    |  9.432    |  1.282e+0 |\n",
      "|  42       | -2.164    |  10.14    |  3.833    |  9.247    |  1.147e+0 |\n",
      "|  43       | -2.164    |  10.33    |  3.402    |  9.888    |  1.158e+0 |\n",
      "|  44       | -2.164    |  10.05    |  3.913    |  9.792    |  1.255e+0 |\n",
      "|  45       | -2.164    |  10.08    |  3.831    |  9.777    |  1.263e+0 |\n",
      "|  46       | -2.164    |  10.6     |  3.393    |  9.863    |  1.136e+0 |\n",
      "|  47       | -2.164    |  10.27    |  3.986    |  9.983    |  1.14e+03 |\n",
      "|  48       | -2.165    |  10.07    |  3.016    |  9.934    |  1.197e+0 |\n",
      "|  49       | -2.165    |  10.21    |  3.27     |  9.914    |  1.232e+0 |\n",
      "|  50       | -2.165    |  10.42    |  3.637    |  9.556    |  1.219e+0 |\n",
      "|  51       | -2.165    |  10.2     |  3.706    |  9.807    |  1.194e+0 |\n",
      "|  52       | -2.164    |  10.7     |  3.858    |  9.883    |  1.148e+0 |\n",
      "=========================================================================\n",
      "Final result: {'target': -2.164137725274226, 'params': {'max_depth': 10.276489459178219, 'min_samples_leaf': 3.924120866975472, 'min_samples_split': 9.777434874961703, 'n_estimators': 1330.1666290176752}}\n"
     ]
    }
   ],
   "source": [
    "# Let's try it out\n",
    "bOpt_rfc(XX_train, yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1178132747615064\n"
     ]
    }
   ],
   "source": [
    "# now make a new random forecst regressor using the optimized paramters\n",
    "\n",
    "rf_bOpt = RandomForestRegressor(n_estimators=1330, bootstrap=True, \n",
    "            max_features='sqrt', max_depth=10, min_samples_leaf=4, min_samples_split=9, random_state=42)\n",
    "\n",
    "rf_bOpt.fit(XX_train, yy_train)\n",
    "yy_pred = rf_bOpt.predict(XX_test)\n",
    "MAE = mean_absolute_error(yy_test, yy_pred)\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vola, thanks for browsing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
